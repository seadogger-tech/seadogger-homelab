---
# Step -2: Handle macOS fork safety
- name: Detect operating system
  ansible.builtin.set_fact:
    is_macos: "{{ ansible_facts['os_family'] == 'Darwin' }}"

- name: Set Python fork safety for macOS
  ansible.builtin.set_fact:
    ansible_python_interpreter: "python3 -E"
    env_vars:
      OBJC_DISABLE_INITIALIZE_FORK_SAFETY: "YES"
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: is_macos | bool

- name: Set Linux environment variables
  ansible.builtin.set_fact:
    env_vars:
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: not is_macos | bool

# Step -1: Nuke the entire Rook-Ceph cluster to ensure a clean slate
- name: Uninstall Rook-Ceph Cluster Helm chart
  ansible.builtin.shell:
    cmd: helm uninstall rook-ceph-cluster -n rook-ceph
  environment: "{{ env_vars }}"
  register: helm_uninstall_cluster_result
  changed_when: "'release \"rook-ceph-cluster\" uninstalled' in helm_uninstall_cluster_result.stdout"
  ignore_errors: true
  when: wipe_ceph_disks_on_install | bool

- name: Uninstall Rook-Ceph Operator Helm chart
  ansible.builtin.shell:
    cmd: helm uninstall rook-ceph -n rook-ceph
  environment: "{{ env_vars }}"
  register: helm_uninstall_operator_result
  changed_when: "'release \"rook-ceph\" uninstalled' in helm_uninstall_operator_result.stdout"
  ignore_errors: true
  when: wipe_ceph_disks_on_install | bool

- name: Force delete all resources in rook-ceph namespace
  ansible.builtin.shell:
    cmd: kubectl delete all --all -n rook-ceph --force --grace-period=0
  environment: "{{ env_vars }}"
  changed_when: true
  ignore_errors: true
  when: wipe_ceph_disks_on_install | bool

- name: Delete rook-ceph namespace
  ansible.builtin.shell:
    cmd: kubectl delete namespace rook-ceph --ignore-not-found --force --grace-period=0
  environment: "{{ env_vars }}"
  changed_when: true
  when: wipe_ceph_disks_on_install | bool

# Step 0: Wipe and validate device partitions (CONDITIONAL)
- name: Wipe Ceph device partitions
  ansible.builtin.shell: |
    set -e
    sgdisk --zap-all {{ hostvars[inventory_hostname]['device'] }}
    dd if=/dev/zero of={{ hostvars[inventory_hostname]['device'] }} bs=1M count=100 conv=fsync
    wipefs -a {{ hostvars[inventory_hostname]['device'] }}
  when:
    - hostvars[inventory_hostname]['device'] is defined
    - wipe_ceph_disks_on_install | bool
  ignore_errors: false

- name: Validate partition is clean
  ansible.builtin.shell: |
    lsblk -f {{ hostvars[inventory_hostname]['device'] }} | grep -Ev '^(NAME|$)' | awk '{print $2}'
  register: partition_check
  changed_when: false
  when:
    - hostvars[inventory_hostname]['device'] is defined
    - wipe_ceph_disks_on_install | bool

- name: Fail if partition still has filesystem or label
  ansible.builtin.fail:
    msg: "Device {{ hostvars[inventory_hostname]['device'] }} is not clean. Filesystem or label found: {{ partition_check.stdout_lines }}"
  when:
    - hostvars[inventory_hostname]['device'] is defined
    - wipe_ceph_disks_on_install | bool
    - partition_check.stdout != ""

# Step 1: Set KUBECONFIG globally
- name: Set KUBECONFIG global
  ansible.builtin.set_fact:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml

# Step 3: Add Helm repo using shell to avoid fork issues
- name: Add Rook Helm repo
  ansible.builtin.shell:
    cmd: helm repo add rook-release https://charts.rook.io/release && helm repo update
  environment: "{{ env_vars }}"
  changed_when: false

# Step 4: Install operator
- name: Install Rook-Ceph Operator
  ansible.builtin.shell:
    cmd: >-
      helm upgrade --install rook-ceph rook-release/rook-ceph
      --namespace rook-ceph
      --create-namespace
      -f https://raw.githubusercontent.com/seadogger/seadogger-homelab/master/helm-deployments/rook-ceph/rook-ceph-operator-values.yaml
      --wait
  environment: "{{ env_vars }}"
  register: helm_operator_result
  changed_when: "'STATUS: deployed' in helm_operator_result.stdout"

# Step 5: Wait for operator to be ready
- name: Wait for operator to be ready
  ansible.builtin.shell:
    cmd: kubectl -n rook-ceph wait --for=condition=ready pod -l app=rook-ceph-operator --timeout=300s
  environment: "{{ env_vars }}"
  register: wait_result
  retries: 10
  delay: 30
  until: wait_result.rc == 0

# Step 6: Install cluster
- name: Install Rook-Ceph Cluster (without filesystem)
  ansible.builtin.shell:
    cmd: >-
      helm upgrade --install rook-ceph-cluster rook-release/rook-ceph-cluster
      --namespace rook-ceph
      -f https://raw.githubusercontent.com/seadogger/seadogger-homelab/master/helm-deployments/rook-ceph/rook-ceph-cluster-values.yaml
      --wait
  environment: "{{ env_vars }}"
  register: helm_cluster_result
  changed_when: "'STATUS: deployed' in helm_cluster_result.stdout"

- name: Display message about Ceph cluster readiness time
  ansible.builtin.debug:
    msg: |
      Waiting for the Ceph cluster to become ready. This may take 15â€“20 minutes depending on your system performance.
      Do not proceed with any other deployments until this step completes successfully.

- name: Wait for ceph-mgr to be available
  ansible.builtin.shell: |
    kubectl -n rook-ceph wait --for=condition=Ready pod -l app=rook-ceph-mgr --timeout=300s
  environment: "{{ env_vars }}"
  register: mgr_ready
  retries: 10
  delay: 10
  until: mgr_ready.rc == 0

- name: Wait until required pools exist
  ansible.builtin.shell: |
    set -e
    kubectl -n rook-ceph exec "$(kubectl -n rook-ceph get pods -l app=rook-ceph-tools -o name | head -n1)" -- \
      sh -lc '
        ceph osd pool ls | grep -qw ceph-blockpool && \
        ceph osd pool ls | grep -qw ec-fs-metadata && \
        ceph osd pool ls | grep -qw ec-fs-data
      '
  register: pools_exist
  retries: 60
  delay: 10
  until: pools_exist.rc == 0
  changed_when: false

- name: Wait for Ceph health to be OK or WARN
  ansible.builtin.shell: |
    kubectl -n rook-ceph exec "$(kubectl -n rook-ceph get pods -l app=rook-ceph-tools -o name | head -n1)" -- \
      sh -lc 'ceph health | grep -Eq "HEALTH_OK|HEALTH_WARN"'
  register: health_ok_or_warn
  retries: 60
  delay: 30
  until: health_ok_or_warn.rc == 0
  changed_when: false

- name: Create CephFilesystem with Replicated and EC pools
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: ceph.rook.io/v1
      kind: CephFilesystem
      metadata:
        name: ec-fs
        namespace: rook-ceph
      spec:
        metadataPool:
          replicated:
            size: 3
        dataPools:
          - name: ec-fs-data
            failureDomain: host
            erasureCoded:
              dataChunks: 2
              codingChunks: 1
            parameters:
              pg_num: "128"
              allow_ec_overwrites: "true"
              bulk: "true"
        metadataServer:
          activeCount: 1
          activeStandby: true

- name: Create CephFS Erasure Coded StorageClass
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: rook-ceph-filesystem-ec
      provisioner: rook-ceph.cephfs.csi.ceph.com
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      parameters:
        clusterID: rook-ceph
        fsName: ec-fs
        pool: ec-fs-data
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

- name: Unset all default StorageClasses
  ansible.builtin.shell: |
    for sc in $(kubectl get storageclass -o jsonpath='{.items[*].metadata.name}'); do
      kubectl patch storageclass "$sc" -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "false"}}}' || true
    done
  environment: "{{ env_vars }}"

- name: Set ceph-block as default StorageClass
  ansible.builtin.shell: |
    kubectl patch storageclass ceph-block -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "true"}}}'
  environment: "{{ env_vars }}"
