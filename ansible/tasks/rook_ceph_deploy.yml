---
# Step 0: Wipe and validate device partitions (CONDITIONAL)
- name: Wipe Ceph device partitions
  ansible.builtin.shell: |
    set -e
    sgdisk --zap-all {{ hostvars[inventory_hostname]['device'] }}
    dd if=/dev/zero of={{ hostvars[inventory_hostname]['device'] }} bs=1M count=100 conv=fsync
    wipefs -a {{ hostvars[inventory_hostname]['device'] }}
  when:
    - hostvars[inventory_hostname]['device'] is defined
    - wipe_ceph_disks_on_install | bool
  ignore_errors: false

- name: Validate partition is clean
  ansible.builtin.shell: |
    lsblk -f {{ hostvars[inventory_hostname]['device'] }} | grep -Ev '^(NAME|$)' | awk '{print $2}'
  register: partition_check
  changed_when: false
  when:
    - hostvars[inventory_hostname]['device'] is defined
    - wipe_ceph_disks_on_install | bool

- name: Fail if partition still has filesystem or label
  ansible.builtin.fail:
    msg: "Device {{ hostvars[inventory_hostname]['device'] }} is not clean. Filesystem or label found: {{ partition_check.stdout_lines }}"
  when:
    - hostvars[inventory_hostname]['device'] is defined
    - wipe_ceph_disks_on_install | bool
    - partition_check.stdout != ""

# Step 1: Set KUBECONFIG globally
- name: Set KUBECONFIG global
  ansible.builtin.set_fact:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml

# Step 2: Handle macOS fork safety
- name: Detect operating system
  ansible.builtin.set_fact:
    is_macos: "{{ ansible_facts['os_family'] == 'Darwin' }}"

- name: Set Python fork safety for macOS
  ansible.builtin.set_fact:
    ansible_python_interpreter: "python3 -E"
    env_vars:
      OBJC_DISABLE_INITIALIZE_FORK_SAFETY: "YES"
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: is_macos | bool

- name: Set Linux environment variables
  ansible.builtin.set_fact:
    env_vars:
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: not is_macos | bool

# Step 3: Add Helm repo using shell to avoid fork issues
- name: Add Rook Helm repo
  ansible.builtin.shell:
    cmd: helm repo add rook-release https://charts.rook.io/release && helm repo update
  environment: "{{ env_vars }}"
  changed_when: false

# Step 4: Install operator
- name: Install Rook-Ceph Operator
  ansible.builtin.shell:
    cmd: >-
      helm upgrade --install rook-ceph rook-release/rook-ceph
      --namespace rook-ceph
      --create-namespace
      -f https://raw.githubusercontent.com/seadogger/seadogger-homelab/master/helm-deployments/rook-ceph/rook-ceph-operator-values.yaml
      --wait
  environment: "{{ env_vars }}"
  register: helm_operator_result
  changed_when: "'STATUS: deployed' in helm_operator_result.stdout"

# Step 5: Wait for operator to be ready
- name: Wait for operator to be ready
  ansible.builtin.shell:
    cmd: kubectl -n rook-ceph wait --for=condition=ready pod -l app=rook-ceph-operator --timeout=300s
  environment: "{{ env_vars }}"
  register: wait_result
  retries: 10
  delay: 30
  until: wait_result.rc == 0

# Step 6: Install cluster
- name: Install Rook-Ceph Cluster
  ansible.builtin.shell:
    cmd: >-
      helm upgrade --install rook-ceph-cluster rook-release/rook-ceph-cluster
      --namespace rook-ceph
      -f https://raw.githubusercontent.com/seadogger/seadogger-homelab/master/helm-deployments/rook-ceph/rook-ceph-cluster-values.yaml
      --wait
  environment: "{{ env_vars }}"
  register: helm_cluster_result
  changed_when: "'STATUS: deployed' in helm_cluster_result.stdout"

- name: Display message about Ceph cluster readiness time
  ansible.builtin.debug:
    msg: |
      Waiting for the Ceph cluster to become ready. This may take 15â€“20 minutes depending on your system performance.
      Do not proceed with any other deployments until this step completes successfully.

- name: Wait for ceph-mgr to be available
  ansible.builtin.shell: |
    kubectl -n rook-ceph wait --for=condition=Ready pod -l app=rook-ceph-mgr --timeout=300s
  environment: "{{ env_vars }}"
  register: mgr_ready
  retries: 10
  delay: 10
  until: mgr_ready.rc == 0

- name: Wait for Ceph health OK or WARN and pools exist
  ansible.builtin.shell: |
    set -e
    kubectl -n rook-ceph exec "$(kubectl -n rook-ceph get pods -l app=rook-ceph-mgr -o name | head -n1)" -- \
      sh -lc 'ceph health | grep -Eq "HEALTH_OK|HEALTH_WARN" \
      && ceph osd pool ls | grep -qw ec-fs-metadata \
      && ceph osd pool ls | grep -qw ec-fs-data'
  environment: "{{ env_vars }}"
  register: ceph_health_check
  retries: 60
  delay: 30
  until: ceph_health_check.rc == 0

- name: Create CephFS Erasure Coded StorageClass
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: rook-ceph-filesystem-ec
      provisioner: rook-ceph.cephfs.csi.ceph.com
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      parameters:
        clusterID: rook-ceph
        fsName: ec-fs
        pool: data
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

- name: Unset all default StorageClasses
  ansible.builtin.shell: |
    for sc in $(kubectl get storageclass -o jsonpath='{.items[*].metadata.name}'); do
      kubectl patch storageclass "$sc" -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "false"}}}' || true
    done
  environment: "{{ env_vars }}"

- name: Set ceph-block as default StorageClass
  ansible.builtin.shell: |
    kubectl patch storageclass ceph-block -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "true"}}}'
  environment: "{{ env_vars }}"

- name: Create Ganesha metadata pool
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: ceph.rook.io/v1
      kind: CephBlockPool
      metadata:
        name: nfs-ganesha-pool
        namespace: rook-ceph
      spec:
        name: .nfs
        replicated:
          size: 3
          requireSafeReplicaSize: true

- name: Create CephNFS server
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: ceph.rook.io/v1
      kind: CephNFS
      metadata:
        name: nfs-ec
        namespace: rook-ceph
      spec:
        rados:
          pool: .nfs
          namespace: nfs-ec
        server:
          active: 1

- name: Discover NFS client secret for nfs-ec
  ansible.builtin.shell: |
    kubectl -n rook-ceph get secret \
      -l app=rook-ceph-nfs,ceph_nfs=nfs-ec \
      -o jsonpath='{.items[0].metadata.name}'
  register: nfs_secret_name
  changed_when: false

- name: Read userID from NFS client secret
  ansible.builtin.shell: |
    kubectl -n rook-ceph get secret "{{ nfs_secret_name.stdout }}" -o jsonpath='{.data.userID}' | base64 --decode
  register: nfs_user_id
  changed_when: false
  failed_when: false

- name: Read userKey/key from NFS client secret
  ansible.builtin.shell: |
    kubectl -n rook-ceph get secret "{{ nfs_secret_name.stdout }}" -o jsonpath='{.data.userKey}' 2>/dev/null \
      | base64 --decode || \
    kubectl -n rook-ceph get secret "{{ nfs_secret_name.stdout }}" -o jsonpath='{.data.key}' \
      | base64 --decode
  register: nfs_user_key
  changed_when: false

- name: Verify cephx caps include the export path
  ansible.builtin.shell: |
    kubectl -n rook-ceph exec "$(kubectl -n rook-ceph get pods -l app=rook-ceph-mgr -o name | head -n1)" -- \
      sh -lc 'ceph auth get "{{ (nfs_user_id.stdout | default("client.nfs.nfs-ec.1")) | trim }}"' | grep -F 'mds' | grep -F 'path=/volumes/nfs/nfs-ec-6t/20af9899-03ef-44f0-afc2-31660f4ce54d'
  environment: "{{ env_vars }}"
  register: caps_check
  changed_when: false
  failed_when: caps_check.rc != 0

- name: Automate RADOS export via ceph-mgr (idempotent)
  ansible.builtin.shell: |
    set -euo pipefail
    mgr() { kubectl -n rook-ceph get pods -l app=rook-ceph-mgr -o name | head -n1; }

    # Create export content *inside* the mgr pod (avoid cp tar races)
    kubectl -n rook-ceph exec "$(mgr)" -- sh -lc 'cat > /tmp/export-100 <<EOF
    EXPORT {
      Export_Id = 100;
      Path = "/volumes/nfs/nfs-ec-6t/20af9899-03ef-44f0-afc2-31660f4ce54d";
      Pseudo = "/ecfs";
      Access_Type = RW;
      Squash = None;
      Protocols = 4;
      Transports = TCP;
      Security_Label = true;
      FSAL {
        Name = CEPH;
        User_Id = "{{ (nfs_user_id.stdout | default('client.nfs.nfs-ec.1')) | trim }}";
        Secret_Access_Key = "{{ nfs_user_key.stdout | trim }}";
        Filesystem = "ec-fs";
      }
      SecType = ["sys"];
    }
    EOF'

    # Put/append using rados from the mgr
    kubectl -n rook-ceph exec "$(mgr)" -- \
      rados -p .nfs --namespace nfs-ec put export-100 /tmp/export-100

    kubectl -n rook-ceph exec "$(mgr)" -- sh -lc '
      set -e
      if ! rados -p .nfs --namespace nfs-ec stat conf-nfs.nfs-ec >/dev/null 2>&1; then
        : > /tmp/conf-nfs.nfs-ec
        rados -p .nfs --namespace nfs-ec put conf-nfs.nfs-ec /tmp/conf-nfs.nfs-ec
      fi
      rados -p .nfs --namespace nfs-ec get conf-nfs.nfs-ec /tmp/conf-nfs.nfs-ec
      grep -q "rados://.nfs/nfs-ec/export-100" /tmp/conf-nfs.nfs-ec || \
        printf "%s\n" "%url \"rados://.nfs/nfs-ec/export-100\"" >> /tmp/conf-nfs.nfs-ec
      rados -p .nfs --namespace nfs-ec put conf-nfs.nfs-ec /tmp/conf-nfs.nfs-ec
    '

    # Show final objects
    kubectl -n rook-ceph exec "$(mgr)" -- \
      rados -p .nfs --namespace nfs-ec ls | sort
  environment: "{{ env_vars }}"
  register: rados_export
  retries: 5
  delay: 6
  until: rados_export.rc == 0

- name: Restart NFS deployment and wait
  ansible.builtin.shell: |
    kubectl -n rook-ceph rollout restart deploy/rook-ceph-nfs-nfs-ec-a
    kubectl -n rook-ceph rollout status  deploy/rook-ceph-nfs-nfs-ec-a --timeout=300s
  environment: "{{ env_vars }}"

- name: Create NFS StorageClass
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: rook-ceph-nfs
      provisioner: rook-ceph.nfs.csi.ceph.com
      reclaimPolicy: Retain
      parameters:
        server: "rook-nfs-loadbalancer.rook-ceph.svc.cluster.local"
        share: "/ecfs"
        clusterID: rook-ceph
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-nfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-nfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

- name: Create NFS LoadBalancer Service
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: v1
      kind: Service
      metadata:
        name: rook-nfs-loadbalancer
        namespace: rook-ceph
        annotations:
          metallb.universe.tf/loadBalancerIPs: "192.168.1.254"
      spec:
        type: LoadBalancer
        selector:
          app: rook-ceph-nfs
          ceph_nfs: nfs-ec
        ports:
        - name: nfs
          port: 2049
          protocol: TCP
          targetPort: 2049
