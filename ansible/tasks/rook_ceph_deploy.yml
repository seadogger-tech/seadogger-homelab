---
# Step 0: Wipe and validate device partitions (CONDITIONAL)
- name: Wipe Ceph device partitions
  ansible.builtin.shell: |
    set -e
    sgdisk --zap-all {{ hostvars[inventory_hostname]['device'] }}
    dd if=/dev/zero of={{ hostvars[inventory_hostname]['device'] }} bs=1M count=100 conv=fsync
    wipefs -a {{ hostvars[inventory_hostname]['device'] }}
  when:
    - hostvars[inventory_hostname]['device'] is defined
    - wipe_ceph_disks_on_install | bool
  ignore_errors: false

- name: Validate partition is clean
  ansible.builtin.shell: |
    lsblk -f {{ hostvars[inventory_hostname]['device'] }} | grep -Ev '^(NAME|$)' | awk '{print $2}'
  register: partition_check
  changed_when: false
  when:
    - hostvars[inventory_hostname]['device'] is defined
    - wipe_ceph_disks_on_install | bool

- name: Fail if partition still has filesystem or label
  ansible.builtin.fail:
    msg: "Device {{ hostvars[inventory_hostname]['device'] }} is not clean. Filesystem or label found: {{ partition_check.stdout_lines }}"
  when:
    - hostvars[inventory_hostname]['device'] is defined
    - wipe_ceph_disks_on_install | bool
    - partition_check.stdout != ""

# Step 1: Set KUBECONFIG globally
- name: Set KUBECONFIG global
  ansible.builtin.set_fact:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml

# Step 2: Handle macOS fork safety
- name: Detect operating system
  ansible.builtin.set_fact:
    is_macos: "{{ ansible_facts['os_family'] == 'Darwin' }}"

- name: Set Python fork safety for macOS
  ansible.builtin.set_fact:
    ansible_python_interpreter: "python3 -E"
    env_vars:
      OBJC_DISABLE_INITIALIZE_FORK_SAFETY: "YES"
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: is_macos | bool

- name: Set Linux environment variables
  ansible.builtin.set_fact:
    env_vars:
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: not is_macos | bool

# Step 3: Add Helm repo using shell to avoid fork issues
- name: Add Rook Helm repo
  ansible.builtin.shell:
    cmd: helm repo add rook-release https://charts.rook.io/release && helm repo update
  environment: "{{ env_vars }}"
  changed_when: false

# Step 4: Install operator
- name: Install Rook-Ceph Operator
  ansible.builtin.shell:
    cmd: >-
      helm upgrade --install rook-ceph rook-release/rook-ceph
      --namespace rook-ceph
      --create-namespace
      -f https://raw.githubusercontent.com/seadogger/seadogger-homelab/master/helm-deployments/rook-ceph/rook-ceph-operator-values.yaml
      --wait
  environment: "{{ env_vars }}"
  register: helm_operator_result
  changed_when: "'STATUS: deployed' in helm_operator_result.stdout"

# Step 5: Wait for operator to be ready
- name: Wait for operator to be ready
  ansible.builtin.shell:
    cmd: kubectl -n rook-ceph wait --for=condition=ready pod -l app=rook-ceph-operator --timeout=300s
  environment: "{{ env_vars }}"
  register: wait_result
  retries: 10
  delay: 30
  until: wait_result.rc == 0

# Step 6: Install cluster
- name: Install Rook-Ceph Cluster
  ansible.builtin.shell:
    cmd: >-
      helm upgrade --install rook-ceph-cluster rook-release/rook-ceph-cluster
      --namespace rook-ceph
      -f https://raw.githubusercontent.com/seadogger/seadogger-homelab/master/helm-deployments/rook-ceph/rook-ceph-cluster-values.yaml
      --wait
  environment: "{{ env_vars }}"
  register: helm_cluster_result
  changed_when: "'STATUS: deployed' in helm_cluster_result.stdout"

- name: Display message about Ceph cluster readiness time
  ansible.builtin.debug:
    msg: |
      Waiting for the Ceph cluster to become ready. This may take 15â€“20 minutes depending on your system performance.
      Do not proceed with any other deployments until this step completes successfully.

- name: Wait for Ceph cluster health to be OK (or WARN) and required pools to exist
  ansible.builtin.shell: |
    set -e
    kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph health | grep -Eq 'HEALTH_OK|HEALTH_WARN'
    kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph osd pool ls | grep -qw ec-fs-metadata
    kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph osd pool ls | grep -qw ec-fs-data
  environment: "{{ env_vars }}"
  register: ceph_health_check
  retries: 60
  delay: 30
  until: ceph_health_check.rc == 0

- name: Create CephFS Erasure Coded StorageClass
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: rook-ceph-filesystem-ec
      provisioner: rook-ceph.cephfs.csi.ceph.com
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      parameters:
        clusterID: rook-ceph
        fsName: ec-fs
        pool: data
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

- name: Unset all default StorageClasses
  ansible.builtin.shell: |
    for sc in $(kubectl get storageclass -o jsonpath='{.items[*].metadata.name}'); do
      kubectl patch storageclass "$sc" -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "false"}}}' || true
    done
  environment: "{{ env_vars }}"

- name: Set ceph-block as default StorageClass
  ansible.builtin.shell: |
    kubectl patch storageclass ceph-block -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "true"}}}'
  environment: "{{ env_vars }}"

- name: Create Ganesha metadata pool
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: ceph.rook.io/v1
      kind: CephBlockPool
      metadata:
        name: nfs-ganesha-pool
        namespace: rook-ceph
      spec:
        name: .nfs
        replicated:
          size: 1
          requireSafeReplicaSize: false

- name: Create CephNFS server
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: ceph.rook.io/v1
      kind: CephNFS
      metadata:
        name: nfs-ec
        namespace: rook-ceph
      spec:
        rados:
          pool: .nfs
          namespace: nfs-ec
        server:
          active: 1

- name: Wait for rook-ceph-tools to be available
  ansible.builtin.shell: |
    kubectl -n rook-ceph rollout status deploy/rook-ceph-tools --timeout=300s
  environment: "{{ env_vars }}"
  register: tools_ready
  retries: 10
  delay: 10
  until: tools_ready.rc == 0

- name: Automate RADOS export with retries
  ansible.builtin.shell: |
    set -e
    cat > /tmp/export-100 <<'EOF'
    EXPORT {
      Export_Id = 100;
      Path = "/volumes/nfs/nfs-ec-6t/20af9899-03ef-44f0-afc2-31660f4ce54d";
      Pseudo = "/ecfs";
      Access_Type = RW;
      Squash = None;
      Protocols = 4;
      Transports = TCP;
      Security_Label = true;

      FSAL {
        Name = CEPH;
        User_Id = "client.nfs.nfs-ec.1";
        Secret_Access_Key = "AQB7YpdoZ0SsEhAAbL9xUDiUDaH4fleOk8WuFw==";
        Filesystem = "ec-fs";
      }
      SecType = ["sys"];
    }
    EOF
    kubectl -n rook-ceph cp /tmp/export-100 $(kubectl -n rook-ceph get pods -l app=rook-ceph-tools -o name | head -n 1):/tmp/export-100
    kubectl -n rook-ceph exec $(kubectl -n rook-ceph get pods -l app=rook-ceph-tools -o name | head -n 1) -- /bin/bash -c '
      rados -p .nfs --namespace nfs-ec put export-100 /tmp/export-100
      rados -p .nfs --namespace nfs-ec get conf-nfs.nfs-ec /tmp/conf-nfs.nfs-ec
      if ! grep -q "%url \"rados://.nfs/nfs-ec/export-100\"" /tmp/conf-nfs.nfs-ec; then
        printf "%s\n" "%url \"rados://.nfs/nfs-ec/export-100\"" >> /tmp/conf-nfs.nfs-ec
        rados -p .nfs --namespace nfs-ec put conf-nfs.nfs-ec /tmp/conf-nfs.nfs-ec
      fi
    '
  environment: "{{ env_vars }}"
  register: rados_export_result
  retries: 5
  delay: 10
  until: rados_export_result.rc == 0

- name: Reload Ganesha pods
  ansible.builtin.shell: |
    kubectl -n rook-ceph rollout restart deployment rook-ceph-nfs-nfs-ec-a
  environment: "{{ env_vars }}"

- name: Create NFS StorageClass
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: rook-ceph-nfs
      provisioner: rook-ceph.nfs.csi.ceph.com
      reclaimPolicy: Retain
      parameters:
        server: "rook-ceph-nfs-ec-nfs.rook-ceph.svc.cluster.local"
        share: "/"
        clusterID: rook-ceph
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-nfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-nfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

- name: Create NFS LoadBalancer Service
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: v1
      kind: Service
      metadata:
        name: rook-nfs-loadbalancer
        namespace: rook-ceph
        annotations:
          metallb.universe.tf/loadBalancerIPs: "192.168.1.254"
      spec:
        type: LoadBalancer
        selector:
          app: rook-ceph-nfs
          ceph_nfs: nfs-ec
        ports:
        - name: nfs
          port: 2049
          protocol: TCP
          targetPort: 2049
