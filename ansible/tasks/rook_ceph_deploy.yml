---
 # Step 0: Wipe and validate device partitions
- name: Wipe Ceph device partitions
  ansible.builtin.shell: |
    set -e
    sgdisk --zap-all {{ hostvars[inventory_hostname]['device'] }}
    dd if=/dev/zero of={{ hostvars[inventory_hostname]['device'] }} bs=1M count=100 conv=fsync
    wipefs -a {{ hostvars[inventory_hostname]['device'] }}
  when: hostvars[inventory_hostname]['device'] is defined
  ignore_errors: false

- name: Validate partition is clean
  ansible.builtin.shell: |
    lsblk -f {{ hostvars[inventory_hostname]['device'] }} | grep -Ev '^(NAME|$)' | awk '{print $2}'
  register: partition_check
  changed_when: false
  when: hostvars[inventory_hostname]['device'] is defined

- name: Fail if partition still has filesystem or label
  ansible.builtin.fail:
    msg: "Device {{ hostvars[inventory_hostname]['device'] }} is not clean. Filesystem or label found: {{ partition_check.stdout_lines }}"
  when:
    - hostvars[inventory_hostname]['device'] is defined
    - partition_check.stdout != ""

# Step 1: Set KUBECONFIG globally
- name: Set KUBECONFIG global
  ansible.builtin.set_fact:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml

# Step 2: Handle macOS fork safety
- name: Detect operating system
  ansible.builtin.set_fact:
    is_macos: "{{ ansible_facts['os_family'] == 'Darwin' }}"

- name: Set Python fork safety for macOS
  ansible.builtin.set_fact:
    ansible_python_interpreter: "python3 -E"
    env_vars:
      OBJC_DISABLE_INITIALIZE_FORK_SAFETY: "YES"
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: is_macos | bool

- name: Set Linux environment variables
  ansible.builtin.set_fact:
    env_vars:
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: not is_macos | bool

# Step 3: Add Helm repo using shell to avoid fork issues
- name: Add Rook Helm repo
  ansible.builtin.shell:
    cmd: helm repo add rook-release https://charts.rook.io/release && helm repo update
  environment: "{{ env_vars }}"
  changed_when: false

# Step 4: Install operator
- name: Install Rook-Ceph Operator
  ansible.builtin.shell:
    cmd: >-
      helm upgrade --install rook-ceph rook-release/rook-ceph
      --namespace rook-ceph
      --create-namespace
      -f https://raw.githubusercontent.com/seadogger/seadogger-homelab/master/helm-deployments/rook-ceph/rook-ceph-operator-values.yaml
      --wait
  environment: "{{ env_vars }}"
  register: helm_operator_result
  changed_when: "'STATUS: deployed' in helm_operator_result.stdout"

# Step 5: Wait for operator to be ready
- name: Wait for operator to be ready
  ansible.builtin.shell:
    cmd: kubectl -n rook-ceph wait --for=condition=ready pod -l app=rook-ceph-operator --timeout=300s
  environment: "{{ env_vars }}"
  register: wait_result
  retries: 10
  delay: 30
  until: wait_result.rc == 0

# Step 6: Install cluster
- name: Install Rook-Ceph Cluster
  ansible.builtin.shell:
    cmd: >-
      helm upgrade --install rook-ceph-cluster rook-release/rook-ceph-cluster
      --namespace rook-ceph
      -f https://raw.githubusercontent.com/seadogger/seadogger-homelab/master/helm-deployments/rook-ceph/rook-ceph-cluster-values.yaml
      --wait
  environment: "{{ env_vars }}"
  register: helm_cluster_result
  changed_when: "'STATUS: deployed' in helm_cluster_result.stdout"

- name: Display message about Ceph cluster readiness time
  ansible.builtin.debug:
    msg: |
      Waiting for the Ceph cluster to become ready. This may take 15â€“20 minutes depending on your system performance.
      Do not proceed with any other deployments until this step completes successfully.

- name: Wait for Ceph cluster health to be OK and EC pool to be present with sufficient capacity
  ansible.builtin.shell: |
    set -e
    # Check Ceph health
    kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph health | grep -q HEALTH_OK

    # Ensure EC pool exists
    kubectl -n rook-ceph exec deploy/rook-ceph-tools -- ceph osd pool ls | grep -qw ec-fs-ec-pool

    # Check erasure coding profile
    kubectl -n rook-ceph exec deploy/rook-ceph-tools -- \
      ceph osd erasure-code-profile get ec-fs-ec-pool_ecprofile | \
      grep -q '^k=2$'
    kubectl -n rook-ceph exec deploy/rook-ceph-tools -- \
      ceph osd erasure-code-profile get ec-fs-ec-pool_ecprofile | \
      grep -q '^m=1$'

    # Check usable size of ec-fs-ec-pool is >= 5 TiB
    AVAILABLE=$(kubectl -n rook-ceph exec deploy/rook-ceph-tools -- \
      ceph df | awk '/ec-fs-ec-pool/ {print $(NF)}' | sed 's/TiB//')
    [ "$(echo "$AVAILABLE >= 5" | bc)" -eq 1 ]
  environment: "{{ env_vars }}"
  register: ceph_health_check
  retries: 60
  delay: 30
  until: ceph_health_check.rc == 0

- name: Unset all default StorageClasses
  ansible.builtin.shell: |
    for sc in $(kubectl get storageclass -o jsonpath='{.items[*].metadata.name}'); do
      kubectl patch storageclass "$sc" -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "false"}}}' || true
    done
  environment: "{{ env_vars }}"

- name: Set rook-ceph-block-storage-ec as default StorageClass
  ansible.builtin.shell: |
    kubectl patch storageclass rook-ceph-block-storage-ec -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "true"}}}'
  environment: "{{ env_vars }}"