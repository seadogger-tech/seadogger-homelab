---
# Step -2: Handle macOS fork safety
- name: Detect operating system
  ansible.builtin.set_fact:
    is_macos: "{{ ansible_facts['os_family'] == 'Darwin' }}"

- name: Set Python fork safety for macOS
  ansible.builtin.set_fact:
    ansible_python_interpreter: "python3 -E"
    env_vars:
      OBJC_DISABLE_INITIALIZE_FORK_SAFETY: "YES"
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: is_macos | bool

- name: Set Linux environment variables
  ansible.builtin.set_fact:
    env_vars:
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: not is_macos | bool

# Step -1: Nuke the entire Rook-Ceph cluster to ensure a clean slate
- name: Uninstall Rook-Ceph Cluster Helm chart
  ansible.builtin.shell:
    cmd: helm uninstall rook-ceph-cluster -n rook-ceph
  environment: "{{ env_vars }}"
  register: helm_uninstall_cluster_result
  changed_when: "'release \"rook-ceph-cluster\" uninstalled' in helm_uninstall_cluster_result.stdout"
  ignore_errors: true

- name: Uninstall Rook-Ceph Operator Helm chart
  ansible.builtin.shell:
    cmd: helm uninstall rook-ceph -n rook-ceph
  environment: "{{ env_vars }}"
  register: helm_uninstall_operator_result
  changed_when: "'release \"rook-ceph\" uninstalled' in helm_uninstall_operator_result.stdout"
  ignore_errors: true

- name: Force delete all resources in rook-ceph namespace
  ansible.builtin.shell:
    cmd: kubectl delete all --all -n rook-ceph --force --grace-period=0
  environment: "{{ env_vars }}"
  changed_when: true
  ignore_errors: true

- name: Delete rook-ceph namespace
  ansible.builtin.shell:
    cmd: kubectl delete namespace rook-ceph --ignore-not-found --force --grace-period=0
  environment: "{{ env_vars }}"
  changed_when: true

# Step 0: Wipe and validate device partitions (CONDITIONAL)
- name: Wipe Ceph device partitions
  ansible.builtin.shell: |
    set -e
    sgdisk --zap-all {{ hostvars[inventory_hostname]['device'] }}
    dd if=/dev/zero of={{ hostvars[inventory_hostname]['device'] }} bs=1M count=100 conv=fsync
    wipefs -a {{ hostvars[inventory_hostname]['device'] }}
  when:
    - hostvars[inventory_hostname]['device'] is defined
    - wipe_ceph_disks_on_install | bool
  ignore_errors: false

- name: Validate partition is clean
  ansible.builtin.shell: |
    lsblk -f {{ hostvars[inventory_hostname]['device'] }} | grep -Ev '^(NAME|$)' | awk '{print $2}'
  register: partition_check
  changed_when: false
  when:
    - hostvars[inventory_hostname]['device'] is defined
    - wipe_ceph_disks_on_install | bool

- name: Fail if partition still has filesystem or label
  ansible.builtin.fail:
    msg: "Device {{ hostvars[inventory_hostname]['device'] }} is not clean. Filesystem or label found: {{ partition_check.stdout_lines }}"
  when:
    - hostvars[inventory_hostname]['device'] is defined
    - wipe_ceph_disks_on_install | bool
    - partition_check.stdout != ""

# Step 1: Set KUBECONFIG globally
- name: Set KUBECONFIG global
  ansible.builtin.set_fact:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml

# Step 3: Add Helm repo using shell to avoid fork issues
- name: Add Rook Helm repo
  ansible.builtin.shell:
    cmd: helm repo add rook-release https://charts.rook.io/release && helm repo update
  environment: "{{ env_vars }}"
  changed_when: false

# Step 4: Install operator
- name: Install Rook-Ceph Operator
  ansible.builtin.shell:
    cmd: >-
      helm upgrade --install rook-ceph rook-release/rook-ceph
      --namespace rook-ceph
      --create-namespace
      -f https://raw.githubusercontent.com/seadogger/seadogger-homelab/master/helm-deployments/rook-ceph/rook-ceph-operator-values.yaml
      --wait
  environment: "{{ env_vars }}"
  register: helm_operator_result
  changed_when: "'STATUS: deployed' in helm_operator_result.stdout"

# Step 5: Wait for operator to be ready
- name: Wait for operator to be ready
  ansible.builtin.shell:
    cmd: kubectl -n rook-ceph wait --for=condition=ready pod -l app=rook-ceph-operator --timeout=300s
  environment: "{{ env_vars }}"
  register: wait_result
  retries: 10
  delay: 30
  until: wait_result.rc == 0

# Step 6: Install cluster
- name: Install Rook-Ceph Cluster
  ansible.builtin.shell:
    cmd: >-
      helm upgrade --install rook-ceph-cluster rook-release/rook-ceph-cluster
      --namespace rook-ceph
      -f https://raw.githubusercontent.com/seadogger/seadogger-homelab/master/helm-deployments/rook-ceph/rook-ceph-cluster-values.yaml
      --wait
  environment: "{{ env_vars }}"
  register: helm_cluster_result
  changed_when: "'STATUS: deployed' in helm_cluster_result.stdout"

- name: Display message about Ceph cluster readiness time
  ansible.builtin.debug:
    msg: |
      Waiting for the Ceph cluster to become ready. This may take 15â€“20 minutes depending on your system performance.
      Do not proceed with any other deployments until this step completes successfully.

- name: Wait for ceph-mgr to be available
  ansible.builtin.shell: |
    kubectl -n rook-ceph wait --for=condition=Ready pod -l app=rook-ceph-mgr --timeout=300s
  environment: "{{ env_vars }}"
  register: mgr_ready
  retries: 10
  delay: 10
  until: mgr_ready.rc == 0

- name: Wait until required CephFS pools exist
  ansible.builtin.shell: |
    set -e
    kubectl -n rook-ceph exec "$(kubectl -n rook-ceph get pods -l app=rook-ceph-mgr -o name | head -n1)" -- \
      sh -lc 'ceph osd pool ls | grep -qw ec-fs-metadata && ceph osd pool ls | grep -qw ec-fs-data'
  register: pools_exist
  retries: 60
  delay: 10
  until: pools_exist.rc == 0
  changed_when: false

- name: Wait for Ceph health to be OK or WARN
  ansible.builtin.shell: |
    kubectl -n rook-ceph exec "$(kubectl -n rook-ceph get pods -l app=rook-ceph-mgr -o name | head -n1)" -- \
      sh -lc 'ceph health | grep -Eq "HEALTH_OK|HEALTH_WARN"'
  register: health_ok_or_warn
  retries: 60
  delay: 30
  until: health_ok_or_warn.rc == 0
  changed_when: false

- name: Create CephFS Erasure Coded StorageClass
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: rook-ceph-filesystem-ec
      provisioner: rook-ceph.cephfs.csi.ceph.com
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      parameters:
        clusterID: rook-ceph
        fsName: ec-fs
        pool: data
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

- name: Unset all default StorageClasses
  ansible.builtin.shell: |
    for sc in $(kubectl get storageclass -o jsonpath='{.items[*].metadata.name}'); do
      kubectl patch storageclass "$sc" -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "false"}}}' || true
    done
  environment: "{{ env_vars }}"

- name: Set ceph-block as default StorageClass
  ansible.builtin.shell: |
    kubectl patch storageclass ceph-block -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "true"}}}'
  environment: "{{ env_vars }}"

- name: Create Ganesha metadata pool
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: ceph.rook.io/v1
      kind: CephBlockPool
      metadata:
        name: nfs-ganesha-pool
        namespace: rook-ceph
      spec:
        name: .nfs
        replicated:
          size: 3
          requireSafeReplicaSize: true

- name: Create CephNFS server
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: ceph.rook.io/v1
      kind: CephNFS
      metadata:
        name: nfs-ec
        namespace: rook-ceph
      spec:
        rados:
          pool: .nfs
          namespace: nfs-ec
        server:
          active: 1

- name: Set NFS export path fact
  ansible.builtin.set_fact:
    nfs_export_path: "/volumes/nfs/nfs-ec-6t/20af9899-03ef-44f0-afc2-31660f4ce54d"
  tags:
    - rook_ceph

- name: Ensure .nfs pool has correct replica size
  ansible.builtin.shell: |
    set -euo pipefail
    MGR_POD=$(kubectl -n rook-ceph get pods -l app=rook-ceph-mgr -o name | head -n1)
    kubectl -n rook-ceph exec "$MGR_POD" -- bash -lc '
      cur=$(ceph osd pool get .nfs size 2>/dev/null | awk "/size:/ {print \$2}" || echo "")
      [ "$cur" = "3" ] || ceph osd pool set .nfs size 3
      curmin=$(ceph osd pool get .nfs min_size 2>/dev/null | awk "/min_size:/ {print \$2}" || echo "")
      [ "$curmin" = "2" ] || ceph osd pool set .nfs min_size 2
    '
  changed_when: false
  tags:
    - rook_ceph

- name: Discover NFS client secret for nfs-ec (with retry)
  ansible.builtin.shell: |
    kubectl -n rook-ceph get secret \
      -l app=rook-ceph-nfs,ceph_nfs=nfs-ec \
      -o jsonpath='{.items[0].metadata.name}'
  register: nfs_secret_name
  changed_when: false
  failed_when: nfs_secret_name.rc != 0 or nfs_secret_name.stdout == ""
  retries: 20
  delay: 6
  tags:
    - rook_ceph

- name: Read userID from NFS client secret
  ansible.builtin.shell: |
    kubectl -n rook-ceph get secret "{{ nfs_secret_name.stdout }}" -o jsonpath='{.data.userID}' | base64 --decode
  register: nfs_user_id
  changed_when: false
  failed_when: nfs_user_id.rc != 0 or nfs_user_id.stdout == ""
  no_log: true
  tags:
    - rook_ceph

- name: Read userKey/key from NFS client secret
  ansible.builtin.shell: |
    set -euo pipefail
    kubectl -n rook-ceph get secret "{{ nfs_secret_name.stdout }}" -o jsonpath='{.data.userKey}' 2>/dev/null \
      | base64 --decode || \
    kubectl -n rook-ceph get secret "{{ nfs_secret_name.stdout }}" -o jsonpath='{.data.key}' \
      | base64 --decode
  register: nfs_user_key
  changed_when: false
  failed_when: nfs_user_key.rc != 0 or nfs_user_key.stdout == ""
  no_log: true
  tags:
    - rook_ceph

- name: Verify cephx caps include the export path and filesystem
  ansible.builtin.shell: |
    kubectl -n rook-ceph exec "$(kubectl -n rook-ceph get pods -l app=rook-ceph-mgr -o name | head -n1)" -- \
      sh -lc 'ceph auth get "{{ nfs_user_id.stdout | trim }}"'
  environment: "{{ env_vars }}"
  register: cephx_user_caps
  changed_when: false
  failed_when: cephx_user_caps.rc != 0 or ('path=' ~ nfs_export_path) not in cephx_user_caps.stdout or 'fs=ec-fs' not in cephx_user_caps.stdout
  no_log: true
  tags:
    - rook_ceph

- name: Automate RADOS export via ceph-mgr (idempotent)
  ansible.builtin.shell: |
    set -euo pipefail
    MGR_POD=$(kubectl -n rook-ceph get pods -l app=rook-ceph-mgr -o name | head -n1)

    # Create export content *inside* the mgr pod to avoid races
    kubectl -n rook-ceph exec "$MGR_POD" -- sh -lc 'cat > /tmp/export-100 <<EOF
    EXPORT {
      Export_Id = 100;
      Path = "/volumes/nfs/nfs-ec-6t/20af9899-03ef-44f0-afc2-31660f4ce54d";
      Pseudo = "/ecfs";
      Access_Type = RW;
      Squash = None;
      Protocols = 4;
      Transports = TCP;
      Security_Label = true;
      SecType = ["sys"];

      FSAL {
        Name = CEPH;
        User_Id = "{{ nfs_user_id.stdout | trim }}";
        Secret_Access_Key = "{{ nfs_user_key.stdout | trim }}";
        fs_name = "ec-fs";
      }
    }
    EOF'

    # Put the export object into RADOS from the mgr pod
    kubectl -n rook-ceph exec "$MGR_POD" -- \
      rados -p .nfs --namespace nfs-ec put export-100 /tmp/export-100

    # Idempotently ensure the main config includes the export URL
    kubectl -n rook-ceph exec "$MGR_POD" -- sh -lc '
      set -e
      # Get current config, or create if non-existent
      rados -p .nfs --namespace nfs-ec get conf-nfs.nfs-ec /tmp/conf-nfs.nfs-ec || : > /tmp/conf-nfs.nfs-ec

      # Add the include URL if it is not already present
      if ! grep -q "rados://.nfs/nfs-ec/export-100" /tmp/conf-nfs.nfs-ec; then
        printf "%s\n" "%url \"rados://.nfs/nfs-ec/export-100\"" >> /tmp/conf-nfs.nfs-ec
      fi

      # Write the updated config back to RADOS
      rados -p .nfs --namespace nfs-ec put conf-nfs.nfs-ec /tmp/conf-nfs.nfs-ec
    '

    # Show final objects for verification
    kubectl -n rook-ceph exec "$MGR_POD" -- \
      rados -p .nfs --namespace nfs-ec ls | sort
  environment: "{{ env_vars }}"
  register: rados_export
  retries: 5
  delay: 6
  until: rados_export.rc == 0
  no_log: true
  tags:
    - rook_ceph

- name: Restart NFS Ganesha pods to apply configuration
  ansible.builtin.command: >
    kubectl delete pod -n rook-ceph -l app=rook-ceph-nfs,ceph_nfs=nfs-ec
  changed_when: true
  tags:
    - rook_ceph

- name: Wait for NFS Ganesha deployment to be ready
  ansible.builtin.shell: |
    kubectl -n rook-ceph wait --for=condition=Available deployment/rook-ceph-nfs-nfs-ec-a --timeout=300s
  environment: "{{ env_vars }}"
  changed_when: false
  tags:
    - rook_ceph

- name: Verify Ganesha loaded export-100 from RADOS
  ansible.builtin.shell: |
    kubectl -n rook-ceph logs deploy/rook-ceph-nfs-nfs-ec-a --since=5m \
    | grep -E 'Export .* created .* pseudo \(/ecfs\)'
  register: ganesha_loaded
  retries: 12
  delay: 5
  until: ganesha_loaded.rc == 0
  changed_when: false
  tags:
    - rook_ceph

- name: Create NFS StorageClass
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: rook-ceph-nfs
      provisioner: rook-ceph.nfs.csi.ceph.com
      reclaimPolicy: Retain
      parameters:
        server: "rook-nfs-loadbalancer.rook-ceph.svc.cluster.local"
        share: "/ecfs"
        clusterID: rook-ceph
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-nfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-nfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

- name: Create NFS LoadBalancer Service
  ansible.builtin.k8s:
    kubeconfig: "{{ KUBECONFIG }}"
    state: present
    definition:
      apiVersion: v1
      kind: Service
      metadata:
        name: rook-nfs-loadbalancer
        namespace: rook-ceph
        annotations:
          metallb.universe.tf/loadBalancerIPs: "192.168.1.254"
      spec:
        type: LoadBalancer
        selector:
          app: rook-ceph-nfs
          ceph_nfs: nfs-ec
        ports:
        - name: nfs
          port: 2049
          protocol: TCP
          targetPort: 2049
