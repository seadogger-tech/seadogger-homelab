operatorNamespace: rook-ceph

cephClusterSpec:
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.0
    allowUnsupported: false

  dataDirHostPath: /var/lib/rook

  mon:
    count: 3
    allowMultiplePerNode: false
    placement:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - anakin.local
              - obiwan.local
              - rey.local

  dashboard:
    enabled: true
    ssl: false

  network:
    provider: host

  # Disable crash collector to save resources
  crashCollector:
    disable: true

  # Disable monitoring
  monitoring:
    enabled: false

  mgr:
    count: 1
    modules:
    - name: balancer
      enabled: true
    placement:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - anakin.local
              - obiwan.local
              - rey.local

resources:
  mgr:
    limits:
      cpu: "1000m"      # 1 core max
      memory: "1Gi"     # Increased for better performance
    requests:
      cpu: "250m"
      memory: "512Mi"   # Increased base allocation

  mon:
    limits:
      cpu: "1000m"      # 1 core max
      memory: "1Gi"     # Increased for stability
    requests:
      cpu: "250m"
      memory: "512Mi"   # Increased base allocation

  osd:
    limits:
      cpu: "2000m"      # 2 cores max - OSDs need more CPU
      memory: "2Gi"     # Increased for better performance
    requests:
      cpu: "500m"
      memory: "1Gi"     # OSDs need more memory

  prepareosd:
    limits:
      cpu: "500m"
      memory: "512Mi"   # Temporary operation, can be higher
    requests:
      cpu: "250m"
      memory: "256Mi"

  mgr-sidecar:
    limits:
      cpu: "500m"
      memory: "512Mi"
    requests:
      cpu: "100m"
      memory: "256Mi"

  storage:
    useAllNodes: false
    useAllDevices: false
    config:
      osdsPerDevice: "1"
      storeType: "bluestore"
      deviceClass: "nvme"
    nodes:
    - name: anakin.local
      devices:
      - name: /dev/nvme0n1p3
        config:
          deviceClass: "nvme"
    - name: obiwan.local
      devices:
      - name: /dev/nvme0n1p3
        config:
          deviceClass: "nvme"
    - name: rey.local
      devices:
      - name: /dev/nvme0n1p3
        config:
          deviceClass: "nvme"

# Explicitly disable Object Store
cephObjectStores: []

cephFileSystems:
  - name: ec-fs
    spec:
      metadataPool:
        replicated:
          size: 3
      dataPools:
        - name: data
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
          parameters:
            pg_num: "128"
            allow_ec_overwrites: "true"
            bulk: "true"
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          limits:
            cpu: "500m"
            memory: "512Mi"
          requests:
            cpu: "100m"
            memory: "256Mi"
        placement:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - anakin.local
                  - obiwan.local
                  - rey.local
    storageClass:  # Move this outside of metadataServer
      enabled: true
      name: rook-ceph-filesystem-ec
      isDefault: true
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      parameters:
        fsName: ec-fs
        # The pool name will be prefixed with the filesystem name, resulting in 'ec-fs-data'
        pool: data
        clusterID: rook-ceph
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph


csi:
  enableCephfsDriver: true
  enableRbdDriver: false
  enableCephfsSnapshotter: false
  enableRBDSnapshotter: true
  enableNFSDriver: false
  
  provisionerResources:
    limits:
      cpu: "200m"
      memory: "256Mi"
    requests:
      cpu: "50m"
      memory: "128Mi"
  
  pluginResources:
    limits:
      cpu: "200m"
      memory: "256Mi"
    requests:
      cpu: "50m"
      memory: "128Mi"

# Disable monitoring completely
monitoring:
  enabled: false

dashboard:
  enabled: true
  ssl: false
  ingress:
    enabled: false

toolbox:
  enabled: true
  resources:
    limits:
      cpu: "200m"
      memory: "256Mi"
    requests:
      cpu: "50m"
      memory: "128Mi"
