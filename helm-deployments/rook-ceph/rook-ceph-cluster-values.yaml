operatorNamespace: rook-ceph

cephClusterSpec:
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.0
    allowUnsupported: false

  dataDirHostPath: /var/lib/rook

  mon:
    count: 3
    allowMultiplePerNode: false
    placement:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - anakin
              - obiwan
              - rey

  dashboard:
    enabled: true
    ssl: false

  network:
    provider: host

  # Disable crash collector to save resources
  crashCollector:
    disable: true

  # Disable monitoring
  monitoring:
    enabled: false

  mgr:
    count: 1
    modules:
    - name: balancer
      enabled: true
    placement:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - anakin
              - obiwan
              - rey

  resources:
    mgr:
      limits:
        cpu: "300m"
        memory: "384Mi"
      requests:
        cpu: "100m"
        memory: "256Mi"
    mon:
      limits:
        cpu: "300m"
        memory: "384Mi"
      requests:
        cpu: "100m"
        memory: "256Mi"
    osd:
      limits:
        cpu: "400m"
        memory: "768Mi"
      requests:
        cpu: "100m"
        memory: "512Mi"
    prepareosd:
      limits:
        cpu: "200m"
        memory: "256Mi"
      requests:
        cpu: "50m"
        memory: "128Mi"
    mgr-sidecar:
      limits:
        cpu: "200m"
        memory: "256Mi"
      requests:
        cpu: "50m"
        memory: "128Mi"

  storage:
    useAllNodes: false
    useAllDevices: false
    config:
      osdsPerDevice: "1"
    nodes:
    - name: obiwan
      devices:
      - name: nvme0n1p3
    - name: anakin
      devices:
      - name: nvme0n1p3
    - name: rey
      devices:
      - name: nvme0n1p3

cephBlockPools:
  - name: ec-pool
    spec:
      failureDomain: host
      erasureCoded:
        dataChunks: 2
        codingChunks: 1
      parameters:
        crushRoot: "default"
        compression: "aggressive"
    storageClass:
      enabled: true
      name: "rook-ceph-ec-block"
      isDefault: true
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      parameters:
        pool: ec-pool
        thick-provisioning: "false"
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        imageFeatures: layering

csi:
  enableCephfsDriver: false
  enableRbdDriver: true
  
  provisionerResources:
    limits:
      cpu: "200m"
      memory: "256Mi"
    requests:
      cpu: "50m"
      memory: "128Mi"
  
  pluginResources:
    limits:
      cpu: "200m"
      memory: "256Mi"
    requests:
      cpu: "50m"
      memory: "128Mi"

# Disable monitoring completely
monitoring:
  enabled: false

dashboard:
  enabled: true
  ssl: false
  ingress:
    enabled: false

toolbox:
  enabled: true
  resources:
    limits:
      cpu: "200m"
      memory: "256Mi"
    requests:
      cpu: "50m"
      memory: "128Mi"