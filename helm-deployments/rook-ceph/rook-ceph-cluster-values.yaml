operatorNamespace: rook-ceph

cephClusterSpec:
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.0
    allowUnsupported: false

  dataDirHostPath: /var/lib/rook

  mon:
    count: 3
    allowMultiplePerNode: false
    placement:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - anakin.local
              - obiwan.local
              - rey.local

  dashboard:
    enabled: true
    ssl: false

  network:
    provider: host

  # Disable crash collector to save resources
  crashCollector:
    disable: true

  # Disable monitoring
  monitoring:
    enabled: false

  mgr:
    count: 1
    modules:
    - name: balancer
      enabled: true
    placement:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - anakin.local
              - obiwan.local
              - rey.local

resources:
  mgr:
    limits:
      cpu: "1000m"      # 1 core max
      memory: "1Gi"     # Increased for better performance
    requests:
      cpu: "250m"
      memory: "512Mi"   # Increased base allocation

  mon:
    limits:
      cpu: "1000m"      # 1 core max
      memory: "1Gi"     # Increased for stability
    requests:
      cpu: "250m"
      memory: "512Mi"   # Increased base allocation

  osd:
    limits:
      cpu: "2000m"      # 2 cores max - OSDs need more CPU
      memory: "2Gi"     # Increased for better performance
    requests:
      cpu: "500m"
      memory: "1Gi"     # OSDs need more memory

  prepareosd:
    limits:
      cpu: "500m"
      memory: "512Mi"   # Temporary operation, can be higher
    requests:
      cpu: "250m"
      memory: "256Mi"

  mgr-sidecar:
    limits:
      cpu: "500m"
      memory: "512Mi"
    requests:
      cpu: "100m"
      memory: "256Mi"

  storage:
    useAllNodes: false
    useAllDevices: false
    config:
      osdsPerDevice: "1"
      storeType: "bluestore"
      deviceClass: "nvme"
    nodes:
    - name: anakin.local
      devices:
      - name: /dev/nvme0n1p3
        config:
          deviceClass: "nvme"
    - name: obiwan.local
      devices:
      - name: /dev/nvme0n1p3
        config:
          deviceClass: "nvme"
    - name: rey.local
      devices:
      - name: /dev/nvme0n1p3
        config:
          deviceClass: "nvme"

# Explicitly disable Object Store
cephObjectStores: []

cephFileSystems:
  - name: ec-fs
    spec:
      metadataPool:
        replicated:
          size: 3
      dataPools:
        - name: data
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
          parameters:
            pg_num: "128"
            allow_ec_overwrites: "true"
            bulk: "true"
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          limits:
            cpu: "500m"
            memory: "512Mi"
          requests:
            cpu: "100m"
            memory: "256Mi"
        placement:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                  - anakin.local
                  - obiwan.local
                  - rey.local


csi:
  enableCephfsDriver: true
  enableRbdDriver: false
  enableCephfsSnapshotter: false
  enableRBDSnapshotter: true
  enableNFSDriver: true
  
  provisionerResources:
    limits:
      cpu: "200m"
      memory: "256Mi"
    requests:
      cpu: "50m"
      memory: "128Mi"
  
  pluginResources:
    limits:
      cpu: "200m"
      memory: "256Mi"
    requests:
      cpu: "50m"
      memory: "128Mi"

# Disable monitoring completely
monitoring:
  enabled: false

cephNFS:
  nfs-ganesha:
    name: ec-nfs
    namespace: rook-ceph
    server:
      active: 1
      placement:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                - anakin.local
                - obiwan.local
                - rey.local
      resources:
        limits:
          cpu: "1000m"
          memory: "1Gi"
        requests:
          cpu: "500m"
          memory: "512Mi"
    ganeshaConfig: |
      NFS_Core_Param {
          NFS_Protocols = 4;
      }
      EXPORT {
          Export_Id = 1;
          Path = "/";
          FSAL {
              Name = CEPH;
              Ceph_fs = "ec-fs";
          }
          Access_Type = RW;
          Squash = None;
          SecType = sys;
      }


dashboard:
  enabled: true
  ssl: false
  ingress:
    enabled: false

toolbox:
  enabled: true
  resources:
    limits:
      cpu: "200m"
      memory: "256Mi"
    requests:
      cpu: "50m"
      memory: "128Mi"
