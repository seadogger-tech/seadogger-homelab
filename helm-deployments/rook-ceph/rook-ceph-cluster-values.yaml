operatorNamespace: rook-ceph

cephClusterSpec:
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.0
    allowUnsupported: false

  dataDirHostPath: /var/lib/rook

  mon:
    count: 3
    allowMultiplePerNode: false
    placement:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                    - anakin.local
                    - obiwan.local
                    - rey.local

  dashboard:
    enabled: true
    ssl: true
    security:
      adminPasswordSecretName: rook-ceph-dashboard-password

  network:
    provider: ""

  crashCollector:
    disable: true

  monitoring:
    enabled: false

  mgr:
    modules:
      - name: balancer
        enabled: true
    placement:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                    - anakin.local
                    - obiwan.local
                    - rey.local

  resources:
    mgr:
      limits:
        cpu: "1"
        memory: "1536Mi"
      requests:
        cpu: "500m"
        memory: "768Mi"
    mon:
      limits:
        cpu: "1"
        memory: "1Gi"
      requests:
        cpu: "500m"
        memory: "512Mi"
    osd:
      limits:
        cpu: "1"
        memory: "2Gi"
      requests:
        cpu: "500m"
        memory: "1Gi"
    prepareosd:
      limits:
        cpu: "500m"
        memory: "512Mi"
      requests:
        cpu: "250m"
        memory: "256Mi"
    mgr-sidecar:
      limits:
        cpu: "500m"
        memory: "512Mi"
      requests:
        cpu: "100m"
        memory: "256Mi"

  storage:
    useAllNodes: false
    useAllDevices: false
    config:
      osdsPerDevice: "1"
      storeType: "bluestore"
      deviceClass: "nvme"
    nodes:
      - name: anakin.local
        devices:
          - name: /dev/nvme0n1p3
            config:
              deviceClass: "nvme"
      - name: obiwan.local
        devices:
          - name: /dev/nvme0n1p3
            config:
              deviceClass: "nvme"
      - name: rey.local
        devices:
          - name: /dev/nvme0n1p3
            config:
              deviceClass: "nvme"

# ────────────────────────────────────────────────────────────────────────────────
# CEPH BLOCK (RBD)
# Pool: ceph-block-data
# StorageClass: ceph-block-data  (default)
# ────────────────────────────────────────────────────────────────────────────────
cephBlockPools:
  - name: ceph-block-data
    spec:
      replicated:
        size: 3
      parameters:
        pg_autoscale_mode: "on"
        target_size_ratio: "0.005"
        pg_num: "16"
        pg_num_min: "8"
      enableRBDStats: true
    storageClass:
      enabled: true
      name: ceph-block-data
      isDefault: true
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      parameters:
        imageFeatures: layering,exclusive-lock,object-map,fast-diff
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        csi.storage.k8s.io/fstype: ext4

# ────────────────────────────────────────────────────────────────────────────────
# CEPH FILESYSTEMS
# Filesystem: ceph-fs
# Pools:
#   - ceph-fs-metadata          (replicated)
#   - ceph-fs-data-replicated   (replicated – default CephFS data pool)
#   - ceph-fs-data-ec           (erasure-coded data pool)
# StorageClasses:
#   - ceph-fs-data-replicated   (primary SC created via values)
#   - ceph-fs-data-ec           (additional SC; see 'additionalStorageClasses' below)
# NOTE on names: we set short pool names (metadata, data-replicated, data-ec).
# Rook prefixes them with the filesystem name, producing the exact final names.
# ────────────────────────────────────────────────────────────────────────────────
cephFileSystems:
  - name: ceph-fs
    spec:
      # Keep pools if the CR is deleted (recommended during bring-up)
      preservePoolsOnDelete: true

      # Replicated metadata pool -> final name: ceph-fs-metadata
      metadataPool:
        name: metadata
        replicated:
          size: 3
        parameters:
          pg_autoscale_mode: "on"
          pg_num: "8"
          pg_num_min: "4"

      # Data pools
      dataPools:
        # Replicated data pool -> final name: ceph-fs-data-replicated
        - name: data-replicated
          replicated:
            size: 3
          parameters:
            pg_autoscale_mode: "on"
            target_size_ratio: "0.005"
            pg_num: "16"
            pg_num_min: "8"

        # Erasure-coded data pool -> final name: ceph-fs-data-ec
        - name: data-ec
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
          parameters:
            pg_autoscale_mode: "on"
            target_size_ratio: "0.005"
            pg_num: "32"
            pg_num_min: "16"
            allow_ec_overwrites: "true"
            bulk: "true"

      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          limits:
            cpu: "200m"
            memory: "256Mi"
          requests:
            cpu: "100m"
            memory: "128Mi"
        placement:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: kubernetes.io/hostname
                      operator: In
                      values:
                        - anakin.local
                        - obiwan.local
                        - rey.local

    # Primary CephFS StorageClass (replicated)
    storageClass:
      enabled: true
      name: ceph-fs-data-replicated
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      parameters:
        clusterID: rook-ceph
        fsName: ceph-fs
        pool: ceph-fs-data-replicated
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

    # Additional CephFS StorageClass (EC)
    # If your chart version supports only one CephFS SC via values,
    # you can still keep this here for alignment and generate a manifest from it.
    additionalStorageClasses:
      - name: ceph-fs-data-ec
        reclaimPolicy: Delete
        allowVolumeExpansion: true
        parameters:
          clusterID: rook-ceph
          fsName: ceph-fs
          pool: ceph-fs-data-ec
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
          csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

csi:
  enableCephfsDriver: true
  enableRbdDriver: true
  enableCephfsSnapshotter: false
  enableRBDSnapshotter: true
  enableNFSDriver: false

  provisionerResources:
    limits:
      cpu: "200m"
      memory: "256Mi"
    requests:
      cpu: "50m"
      memory: "128Mi"

  pluginResources:
    limits:
      cpu: "200m"
      memory: "256Mi"
    requests:
      cpu: "50m"
      memory: "128Mi"

monitoring:
  enabled: false

toolbox:
  enabled: true
  resources:
    limits:
      cpu: "200m"
      memory: "256Mi"
    requests:
      cpu: "50m"
      memory: "64Mi"
