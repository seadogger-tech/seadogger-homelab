operatorNamespace: rook-ceph

cephClusterSpec:
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.0
    allowUnsupported: false

  dataDirHostPath: /var/lib/rook

  mon:
    count: 3
    allowMultiplePerNode: false

  dashboard:
    enabled: true
    ssl: false
    security:
      adminPasswordSecretName: rook-ceph-dashboard-password

  network:
    provider: ""

  crashCollector:
    disable: true

  monitoring:
    enabled: false

  mgr:
    modules:
      - name: balancer
        enabled: true

  placement:
    mon:
      tolerations:
        - key: "CriticalAddonsOnly"
          operator: "Exists"
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                    - anakin.local
                    - obiwan.local
                    - rey.local
    mgr:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                    - anakin.local
                    - obiwan.local
                    - rey.local

  resources:
    mgr:
      limits:
        cpu: "1000m"
        memory: "1Gi"
      requests:
        cpu: "250m"
        memory: "512Mi"
    mon:
      limits:
        cpu: "1000m"
        memory: "1Gi"
      requests:
        cpu: "250m"
        memory: "512Mi"
    osd:
      limits:
        cpu: "2000m"
        memory: "2Gi"
      requests:
        cpu: "500m"
        memory: "1Gi"
    prepareosd:
      limits:
        cpu: "500m"
        memory: "512Mi"
      requests:
        cpu: "250m"
        memory: "256Mi"
    mgr-sidecar:
      limits:
        cpu: "500m"
        memory: "512Mi"
      requests:
        cpu: "100m"
        memory: "256Mi"

  storage:
    useAllNodes: false
    useAllDevices: false
    config:
      osdsPerDevice: "1"
      storeType: "bluestore"
      deviceClass: "nvme"
    nodes:
      - name: anakin.local
        devices:
          - name: /dev/nvme0n1p3
            config:
              deviceClass: "nvme"
      - name: obiwan.local
        devices:
          - name: /dev/nvme0n1p3
            config:
              deviceClass: "nvme"
      - name: rey.local
        devices:
          - name: /dev/nvme0n1p3
            config:
              deviceClass: "nvme"

# RBD pool + default StorageClass
cephBlockPools:
  - name: ceph-blockpool
    spec:
      replicated:
        size: 3
      enableRBDStats: true
    storageClass:
      enabled: true
      name: ceph-block
      isDefault: true
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      parameters:
        imageFeatures: layering,exclusive-lock,object-map,fast-diff
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        csi.storage.k8s.io/fstype: ext4

# CephFS (create here OR via your playbook, not both)
cephFileSystems:
  - name: ec-fs
    spec:
      metadataPool:
        replicated:
          size: 3
      dataPools:
        - name: data
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1
          parameters:
            pg_num: "128"
            allow_ec_overwrites: "true"
            bulk: "true"
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          limits:
            cpu: "500m"
            memory: "512Mi"
          requests:
            cpu: "100m"
            memory: "128Mi"
        placement:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: kubernetes.io/hostname
                      operator: In
                      values:
                        - anakin.local
                        - obiwan.local
                        - rey.local
    storageClass:
      enabled: false

csi:
  enableCephfsDriver: true
  enableRbdDriver: true          # flipped to true so the ceph-block SC works
  enableCephfsSnapshotter: false
  enableRBDSnapshotter: true
  enableNFSDriver: true

  provisionerResources:
    limits:
      cpu: "200m"
      memory: "256Mi"
    requests:
      cpu: "50m"
      memory: "128Mi"

  pluginResources:
    limits:
      cpu: "200m"
      memory: "256Mi"
    requests:
      cpu: "50m"
      memory: "128Mi"

monitoring:
  enabled: false

dashboard:
  enabled: true
  ssl: false
  ingress:
    enabled: false

toolbox:
  enabled: true
  resources:
    limits:
      cpu: "200m"
      memory: "256Mi"
    requests:
      cpu: "50m"
      memory: "128Mi"
