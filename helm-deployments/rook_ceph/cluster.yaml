apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  dataDirHostPath: /var/lib/rook
  mon:
    count: 1
    allowMultiplePerNode: true
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.0
    allowUnsupported: true
    imagePullPolicy: Always
  dashboard:
    enabled: true
    ssl: false
  network:
    provider: host
  crashCollector:
    disable: true
  monitoring:
    enabled: false
  storage:
    useAllNodes: false
    useAllDevices: false
    nodes:
      - name: "obiwan"
        devices:
          - name: "nvme0n1p3"
      - name: "anakin"
        devices:
          - name: "nvme0n1p3"
      - name: "red"
        devices:
          - name: "nvme0n1p3"
    # Define erasure coding pools here
    erasureCodePools:
      - pool: ec-data
        crushRoot: default
        erasureCodeProfile: ec-profile
    # Optional: If metadata storage is required with replication
    metadataPool:
      pool: ec-metadata
      erasureCodeProfile: ec-profile
      replication:
        size: 3
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/arch
              operator: In
              values:
              - arm64
      tolerations:
        - operator: "Exists"